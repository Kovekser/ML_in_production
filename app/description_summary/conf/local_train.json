{
  "train_file": "train.json",
  "test_file": "test.json",
  "path_to_data": "./data/summaries_fine_tuning",

  "model_id": "meta-llama/Llama-3.2-3B-Instruct",
  "lora_r": 8,
  "lora_alpha": 16,
  "lora_dropout": 0.05,


  "output_dir": "./Llama-3.2-3B-summaries",
  "eval_strategy": "epoch",
  "num_train_epochs": 3,
  "do_eval": true,
  "optim": "adamw_torch",
  "per_device_train_batch_size": 1,
  "per_device_eval_batch_size": 1,
  "gradient_accumulation_steps": 4,
  "gradient_checkpointing": true,
  "learning_rate": 0.0001,

  "warmup_ratio": 0.1,

  "save_steps": 500,
  "seed": 42,
  "bf16": false,
  "fp16": false,
  "report_to": [
    "wandb"
  ],
  "lr_scheduler_type": "cosine",
  "log_level" : "debug",
  "logging_strategy": "steps",
  "logging_first_step": true,
  "logging_steps": 10,
  "eval_on_start": true,
  "save_strategy": "epoch"
}